{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining and Statistics\n",
    "## Session 4 - Regression Analysis\n",
    "*Peter Stikker - Haarlem, the Netherlands*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. 'Simple' Bivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started lets import some libraries that we will definitely need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy as np\n",
    "try:\n",
    "    import numpy as np\n",
    "    print('NumPy already installed, only imported')\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    print('NumPy was not installed, installed and imported')\n",
    "      \n",
    "# pyplot as plt\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot already installed, only imported')\n",
    "except:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    print('PyPlot was not installed, installed and imported')\n",
    "\n",
    "# statsmodels as sm    \n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    print('statsmodels already installed, only imported')\n",
    "except:\n",
    "    !pip install statsmodels\n",
    "    import statsmodels.api as sm\n",
    "    print('statsmodels was not installed, installed and imported')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin with a simple example. We have two variables (x and y), each with some scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "y = np.array([1,3,2,5,7,8,8,9,10,12])\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the best possible straight line to represent these points. That's the challenge in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Finding the best line 'by hand'\n",
    "Any straight line can be written as:\n",
    "$y = b_0 + b_1x$\n",
    "\n",
    "The $b_0$ is known as the constant or intercept and the $b_1$ as the slope or gradient. Both $b_0$ and $b_1$ are sometimes also referred to as coefficients, and sometimes only $b_1$ is deemed a coefficient.\n",
    "\n",
    "We want to choose $b_0$ and $b_1$ in such a way that it minimizes the total difference with the known points. However, not just the normal difference, but actually the squared difference.\n",
    "\n",
    "We could keep on guessing but some smart people did some math for us and came up with two scary looking formulas:\n",
    "\n",
    "\\begin{equation*}\n",
    "b_1=\\frac{\\bar{xy}-\\bar{x}\\times\\bar{y}}{s_x^2}\n",
    "\\end{equation*}\n",
    "\n",
    "And\n",
    "\n",
    "\\begin{equation*}\n",
    "b_0=\\bar{y}-\\bar{x}\\times b_1\n",
    "\\end{equation*}\n",
    "\n",
    "A symbol with a bar on top, simply means average (mean). The $\\bar{xy}$ is the mean of the $x$ values multiplied with $y$, the $s_x^2$ is the variance (the square of the standard deviation).\n",
    "\n",
    "$$s_x^2=\\frac{\\sum_{i=1}^n \\left(x_i - \\bar{x}\\right)^2}{n-1}$$\n",
    "\n",
    "So lets calculate these for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx2 = x.var()\n",
    "mxy = np.array(x*y).mean()\n",
    "b1 = (mxy - x.mean()*y.mean())/sx2\n",
    "print(\"The gradient (b1): \", b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = y.mean() - b1*x.mean()\n",
    "print(\"The constant (b0): \",b0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our linear regression equation is:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = 1.24+ 1.17x\n",
    "\\end{equation*}\n",
    "\n",
    "Now lets calculate our predicted values with these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPrediction = b0 + b1*x\n",
    "\n",
    "plt.scatter(x,y, color='blue')\n",
    "plt.plot(x,myPrediction, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. How good is the best?\n",
    "\n",
    "How well does our prediction actually work. We could of course simply determine the mean of the differences (the so-called residuals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y-myPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem right. We are not far off with the prediction, but this seems ridiculous low. The reason are the negative values, we simply want the difference in absolute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = np.mean(np.absolute(y-myPrediction))\n",
    "print(\"Mean Absolute Error (MAE): \", MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is sometimes known as the Mean Absolute Error (MAE). On average the difference (positive of negative) between our predicted value and the actual value was 0.62.\n",
    "\n",
    "As we saw in the previous session squaring in stead of absolute values is more common in statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.mean((y-myPrediction)**2)\n",
    "print(\"Mean Squared Error (MSE): \", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Mean Squared Error (MSE). On average the squared difference (positive or negative) between our predicted value and the actual value was 0.56.\n",
    "\n",
    "As with the standard deviation, we can take the square root out of this to get rid of this 'squared' in the definition. This is known as the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE=MSE**(0.5)\n",
    "print(\"Root Mean Squared Error (RMSE): \",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more common to indicate how well a model is predicting the data a so-called coefficient of determination is calculated. This is usually written as $r^2$. You might recognize that $r$ from the previous session, it was the correlation coefficient. One way of calculating the determination coefficient is indeed by simply squaring the correlation coefficient.\n",
    "\n",
    "The coefficient of determination will always be between 0 and 1. It is a percentage of the variance in the dependent variable ($y$) that is predictable from the independent variable(s) ($x$).\n",
    "\n",
    "The formula for the correlation coefficient is usually given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "r=\\frac{s_{xy}}{s_x\\times s_y}\n",
    "\\end{equation*}\n",
    "\n",
    "Here $s_{xy}$ is used to indicate the covariance, which in itself can be determined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "s_{xy}=\\frac{\\sum(x-\\bar{x})\\times(y-\\bar{y})}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "I'm using here everywhere $s$ which usually is used for a so-called sample standard deviation. This divides by 'n'. However python more often uses $\\sigma$ which is the population standard deviation. In this case it doesn't really matter since they actually will cancel each other out (the n-1 in the covariance will be cancelling the n-1 in the two standard deviations), and especially with big data using n or n - 1 will not lead to a big difference.\n",
    "\n",
    "Okay, lets calculate that correlation coefficient and determination coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covxy = np.sum((x - x.mean())*(y - y.mean()))/x.size\n",
    "sX = x.std()\n",
    "sY = y.std()\n",
    "\n",
    "cor = covxy/(sX*sY)\n",
    "print('Pearson Correlation Coefficient: ',cor)\n",
    "\n",
    "det = cor**2\n",
    "print('Coefficient of Determination: ', det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation above for the determination coefficient shows the link between the correl and the determination coefficient. However there are other formulas that lead to the same result.\n",
    "\n",
    "The determination coefficient is a percentage. If we wouldn't have had the x-values, our best guess for y would simply be the mean of the y-values. This would result in the following sum of squared differences.\n",
    "\n",
    "\\begin{equation*}\n",
    "SS_{tot}=\\sum(y-\\bar{y})^2\n",
    "\\end{equation*}\n",
    "\n",
    "Now compare this with the squared difference with our predicted values, if we would know the x-values. In this equation $\\hat{y}$ are the predicted values. :\n",
    "\n",
    "\\begin{equation*}\n",
    "SS_{res}=\\sum(y - \\hat{y})^2\n",
    "\\end{equation*}\n",
    "\n",
    "We then divide the two we get the percentage of unexplained variance, or in other words, the percentage of variation that is left if we know the x-values:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{SS_{res}}{SS_{tot}}\n",
    "\\end{equation*}\n",
    "\n",
    "Since the determinaton coefficient is the percentage of explained variance, i.e. how much variation we can explain if we do know the x-values compared to simply guessing the mean, we get the determination coefficient:\n",
    "\n",
    "\\begin{equation*}\n",
    "r^2=1-\\frac{SS_{res}}{SS_{tot}}\n",
    "\\end{equation*}\n",
    "\n",
    "Using Python we can check this. A small trick makes the calculations a little easier. We already have the standard deviation of y, and then we can use that $SS_y=s_y^2\\times n$. So we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - np.sum((y-myPrediction)**2)/(sY**2*y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, you don't have to remember all those formulas. Numpy has you covered. It has a function to determine the correlation coefficient; <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\">**corrcoeff()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a square matrix of 2x2. It shows the correlation coefficients between all possible pairs. So the 1 in the upper left corner is the correlation between x and x. The 0.97... is the correlation between x and y, and then in the next row we have the correlation between y and x, and finally between y and y. The diagonal will always be 1s.\n",
    "\n",
    "Just to extract the correlation coefficient and get the determination coefficient is fairly easy now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)[0,1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we had before, an extremely small difference which we'll consider a rounding error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Using sklearn\n",
    "\n",
    "Of course there have been others who have done this work for us. \n",
    "We could for example use <a href=\"https://scikit-learn.org/\">*sklearn*</a> (you'd have to install this first) and then you can import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "try:\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    print('sklearn already installed, only imported')\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    print('sklearn was not installed, installed and imported')\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does require to <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.reshape.html\">**reshape**</a> our x variable. At the moment, we have only a 'row' of values, with re-shape we can make this into a vector (i.e. one column and each row the value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xRes = x.reshape((-1,1))\n",
    "yRes = y.reshape((-1,1))\n",
    "\n",
    "print('shape of x:', x.shape)\n",
    "print('shape after reshape:' ,xRes.shape)\n",
    "xRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the regression analysis and saving the predicted results we can use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">**LinearRegression()**</a> function from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(xRes,yRes)\n",
    "yPred = model.predict(xRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder all the values we have calculated so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The slope (b1): ',b1)\n",
    "print('The intercept (b0): ',b0)\n",
    "print('Mean Absolute Error:', MAE)\n",
    "print('Mean Squared Error: ', MSE)\n",
    "print('Root Mean Squared Error: ', RMSE)\n",
    "print('Coefficient of determination: ',det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see and compare the result with using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1V2=model.coef_[0]\n",
    "print('The slope (b1): ',b1V2[0])\n",
    "b0V2=model.intercept_\n",
    "print('The intercept (b0): ',b0V2[0])\n",
    "MAE2=metrics.mean_absolute_error(yRes,yPred)\n",
    "print('Mean Absolute Error:', MAE2)\n",
    "MSE2=metrics.mean_squared_error(yRes,yPred)\n",
    "print('Mean Squared Error: ', MSE2)\n",
    "RMSE2=metrics.mean_squared_error(yRes,yPred, squared=False)\n",
    "print('Root Mean Squared Error: ', RMSE2)\n",
    "det2=metrics.r2_score(yRes,yPred)\n",
    "print('Coefficient of determination: ',det2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small difference with our 'manual' formula for b1 and b0. We'll leave that as a rounding error :-)\n",
    "\n",
    "You might notice I've used an index for the b1 coefficient, since we can actually also have multiple variables to use for our prediction. More on this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Exercise\n",
    "On Moodle you will find a file Soccer2019C.csv. We want to predict the Overall score of players solely based on their age. To load the data we can use pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    print('pandas already installed, only imported')\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    print('pandas was not installed, installed and imported')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pandas is imported we can read a file as a pandas dataframe. If your file is in a separate folder 'data' we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF=pd.read_csv('Soccer2019C.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded you can get a quick overview using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your exercise is to find the linear regression equation to predict the Overall score, based on the age.\n",
    "\n",
    "There are different ways you can do this:\n",
    "1. Manually\n",
    "2. Using the sklearn library\n",
    "3. Using the statsmodels.api (not discussed yet)\n",
    "\n",
    "You might have to convert the panda dataframe into a numpy array first. Try to find the regression equation with one (or even more to see if they all say the same).\n",
    "\n",
    "Other things you could do if you think you're done....\n",
    "* Add a visualisation\n",
    "* Create a Python function to perform the manual calculations\n",
    "* Find out which variable has the strongest determination coefficient to predict the Overall score\n",
    "* Find out which two variables (one as predictor (x), one as predicted (y)) will have the strongest determination coefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert the dataframe to an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The manual calculation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using sklearn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using statsmodels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python function to make a prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable with the strongest determination coefficient to predict the Overall score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**which two variables (one as predictor (x), one as predicted (y)) will have the strongest determination coefficient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>BACK TO THE SLIDES</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you have seen how linear regression works. Notice that we only used one variable (x) to predict another variable (y). The variable we are trying to predict (y) is known as the '**dependent variable**' or '**outcome variable**', while the variable used to predict this is the '**independent variable**' or '**predictor**', or '**covariate**', or '**feature**'. Sigh, what a terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Basics of Multiple Linear Regression\n",
    "\n",
    "Instead of trying to predict one variable on just one thing, we can of course also try to predict one variable based on multiple variables. The concept remains the same, but there will be a few additional worries.\n",
    "\n",
    "The multiple linear regression equation can be written as you probably suspect:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y}=b_0+b_1 x_1+b_2 x_2+...+b_n x_n\n",
    "\\end{equation*}\n",
    "\n",
    "Okay, so a new example. We have some interest rates, unemployment rates and want to use those to predict a stock index. Here's the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Int_Rate = np.array([2.75,2.5,2.5,2.5,2.5,2.5,2.5,2.25,2.25,2.25,2,2,2,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75,1.75]).reshape((-1,1))\n",
    "Unemp_Rate = np.array([5.3,5.3,5.3,5.3,5.4,5.6,5.5,5.5,5.5,5.6,5.7,5.9,6,5.9,5.8,6.1,6.2,6.1,6.1,6.1,5.9,6.2,6.2,6.1]).reshape((-1,1))\n",
    "Stock_Index = np.array([1464,1394,1357,1293,1256,1254,1234,1195,1159,1167,1130,1075,1047,965,943,958,971,949,884,866,876,822,704,719]).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets look how well each of the two independent variables (Int_Rate and Unemp_Rate) would predict the Stock_Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(xVal, yVal):\n",
    "    model = LinearRegression().fit(xVal,yVal)\n",
    "    yPred = model.predict(xVal)\n",
    "    b1V2=model.coef_[0]\n",
    "    print('The slope (b1): ',b1V2)\n",
    "\n",
    "    b0V2=model.intercept_\n",
    "    print('The intercept (b0): ',b0V2[0])\n",
    "    det2=metrics.r2_score(yVal,yPred)\n",
    "    print('Coefficient of determination: ',det2)\n",
    "\n",
    "linearRegression(Int_Rate.reshape(-1,1), Stock_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegression(Unemp_Rate.reshape(-1,1), Stock_Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of them doesn't appear to be bad. Now to use both of them to predict the Stock Index, we need to use the <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.hstack.html\">**hstack()**</a> function of numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiX=np.hstack((Int_Rate,Unemp_Rate))\n",
    "multiX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegression(multiX, Stock_Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice the determination coefficient is now better than the two seperate ones.\n",
    "\n",
    "Unfortunately it isn't that simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Adjusted Determination Coefficient\n",
    "\n",
    "Adding any extra variable will always increase the $r^2$ (an example can be found at https://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables). One thing therefor is to adjust the the $r^2$.\n",
    "\n",
    "This can be done using the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{r}^2=1-(1-r^2)\\frac{n-1}{n-p-1}\n",
    "\\end{equation*}\n",
    "\n",
    "In this equation $n$ is the sample size, and $p$ is the number of independent variables (so not counting the constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiModel = LinearRegression().fit(multiX, Stock_Index)\n",
    "yPred = multiModel.predict(multiX)\n",
    "det=metrics.r2_score(Stock_Index,yPred)\n",
    "\n",
    "adjDet=1-(1-det)*(Stock_Index.shape[0]-1)/(Stock_Index.shape[0]-multiX.shape[1]-1)\n",
    "\n",
    "print(adjDet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, still better than the best one single but not as much anymore than before.\n",
    "\n",
    "Another package (api in this case) that can produce a lot more information is the <a href=\"https://www.statsmodels.org/\">*statsmodels*</a> api. You will first have to install it using: conda install -c conda-forge statsmodels\n",
    "\n",
    "Then you can use it as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newX=sm.add_constant(multiX) #add the constant\n",
    "newModel=sm.OLS(Stock_Index,newX).fit()\n",
    "newPred=newModel.predict(newX)\n",
    "newModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>BACK TO THE SLIDES</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Multiple Independent Variables\n",
    "There are two other issues when dealing with multiple independent variables. One is if you start looking for which has the biggest impact, and something known as multicolinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Standardized Coefficients\n",
    "If we look at the example result we had the following coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0=multiModel.intercept_\n",
    "print('The intercept (b0): ',b0[0])\n",
    "    \n",
    "b1=multiModel.coef_[0][0]\n",
    "print('b1: ',b1)\n",
    "\n",
    "b2=multiModel.coef_[0][1]\n",
    "print('b2: ',b2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our equation would then be:\n",
    "\\begin{equation*}\n",
    "\\hat{y}=1798.40+345.54\\times IntRate -250.15\\times UnempRate\n",
    "\\end{equation*}\n",
    "\n",
    "This might suggest that *Int_Rate* has a bigger effect than *Unemp_Rate* just because 345.54 > 250.15.\n",
    "\n",
    "This would be an unfair comparison though. If I were to predict a grade of lets say Mathematics. These were on a scale of 0 to 100. I'm going to predict the Math grades, based on the grades for UML (scale of 0 to 10) and the grades for ITSM (scale of 0 to 100). Without any calculation I'd estimate that the UML grade will have to get multiplied by 10 to rescale it to the 0-100 scale, so the coefficient for UML will probably be higher than the one for ITSM.\n",
    "\n",
    "To accound for this, there is something known as '**standardized coefficients**', these take the scale of the variables used into consideration.\n",
    "\n",
    "To get them, we need to convert the data into so-called z-scores, for which we can use scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from scipy.stats.mstats import zscore\n",
    "    print('scipy already installed, only imported')\n",
    "except:\n",
    "    !pip install scipy\n",
    "    from scipy.stats.mstats import zscore\n",
    "    print('scipy was not installed, installed and imported')    \n",
    "\n",
    "sm.OLS(zscore(Stock_Index), zscore(multiX)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now note that the coef for x1 is 0.5731 and for x2 is -0.3917. Comparing the absolute values of these suggest that x1 (Int_Rate) has a bigger impact than Unemp_Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>BACK TO THE SLIDES</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Multicollineairy\n",
    "\n",
    "Multicollinearity means that one or more of the independent variables (one of the x's) can actually be very accuratly predicted by the other independent variables. If this is the case we can simplify our model by removing that variable.\n",
    "\n",
    "There are a few different ways of detecting multicolinearity (see https://en.wikipedia.org/wiki/Multicollinearity#Detection_of_multicollinearity as a nice starting point). One of them is to use so called Variance Inflation Factors. If this is above 10 you can probably better leave the variable out.\n",
    "\n",
    "We won't go into the details of the calculation. Here's how statsmodels can help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[variance_inflation_factor(newX, j) for j in range(newX.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we used *newX* because we are including the constant in our model (which actually is another choice to consider in regression analysis).\n",
    "\n",
    "The first one we can ignore, it is the VIF of the constant and doesn't have a clear interpretation.\n",
    "\n",
    "The VIF of the other two variables are the same, since the only comparison we are doing is between those two variables. If we'd had three or more variables to predict it would have been different.\n",
    "\n",
    "It is below 10 though, so we're good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>BACK TO THE SLIDES</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3 Holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating a model it is adviced not to use all your data. The problem of using all data, is that you might create a model that works very well with your data, but as soon as you have to apply it to new data the model no longer works. A basic method is to create the model with part of your data, and then test if the model also works with the data you didn't use. There are two main methods to split your data: \n",
    "- **Holdout** \n",
    "- **K-fold (Cross-validation)**  \n",
    "\n",
    "Both methods are using a test set (this is data you did not use before) to evaluate model performance. In this course we'll use the *holdout* method, but if you are interested you can of course go the extra mile and investigate the k-fold method. \n",
    "\n",
    "The purpose of holdout evaluation is to test a model on different data than it was trained on. \n",
    "In this method, the dataset is randomly divided into two or three subsets:\n",
    "\n",
    "-\t**Training**\n",
    "-\t**(Cross)-Validation** \n",
    "-\t**Test** or Holdout set, or “unseen” data\n",
    "\n",
    "Uhm, yes it is annoying that within the holdout technique they use the term 'cross validation' as well, as it is also the name used sometimes for the k-fold technique.\n",
    "\n",
    "So the *training* set probably speaks for itself. It is the data we use to create our model. However, we might have two or more models that all seem to work very well. Then, its time to use our *cross validation* data. This can help in deciding which of those might work best. Once we have a final decision we can test how well the model works with unseen data, using the *test* data set.\n",
    "\n",
    "Other uses of the cross-validation set is to determine some so-called hyper-parameters of a model. But in general it means that the cross-validation set is sometimes seen by the model. The test set is never seen in the process of creating the model, and only used in the final stage to test the model.\n",
    "\n",
    "An often seen division is 60%-20%-20% but there are many debates on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn to set a sample of our data as test data, and part as validation. Similar as you've seen in Computer Vision. Lets use the soccer data as an example.\n",
    "\n",
    "First, just to be sure, reload the data and now set our independent variable to just all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF=pd.read_csv('Soccer2019C.csv')\n",
    "\n",
    "X = soccerDF[['Age', 'Potential', 'ValueInK', \n",
    "       'Special', 'International Reputation', 'Weak Foot',\n",
    "       'Skill Moves', 'Crossing',\n",
    "       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n",
    "       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n",
    "       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n",
    "       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n",
    "       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n",
    "       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n",
    "       'GKKicking', 'GKPositioning', 'GKReflexes']]\n",
    "y = soccerDF['Overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the train_test_split option from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split the data in a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test_size parameter is a proportion of the original dataset to be selected as the test set.\n",
    "\n",
    "The random_state parameter sets the random seed to a specific state so we can replicate the results if needed.\n",
    "\n",
    "We can verify if the test set is indeed 40% of the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.4*X.shape[0])\n",
    "X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create our model. Make sure to use the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel=LinearRegression().fit(X_train,y_train) # To train the model. Only fit the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a clue about the coefficients and the constant/intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept:', myModel.intercept_)\n",
    "print('Coefficients',myModel.coef_)\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, that doesn't look very user-friendly. Panda to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(                                  # creating a dataframe \n",
    "                        myModel.coef_,                    # myModel.coef_ as the data\n",
    "                        X.columns,                        # X.columns as the index\n",
    "                        columns=['Coefficient'])          # column name is 'Coefficient' \n",
    "                                                                    \n",
    "coeff_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big question is of course if all these coefficients are really needed.\n",
    "\n",
    "We could look at the standardized coefficients, and perhaps the Variance Inflation Factors. Lets use another technique: Permutation importance, or 'Mean Decrease Accuracy', which actually uses a Random Forests approach (https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html).\n",
    "\n",
    "Tricky stuff, so import a package to do the work for us: eli5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import eli5 \n",
    "    print('eli5 already installed, only imported')\n",
    "except:\n",
    "    !pip install eli5\n",
    "    import eli5 \n",
    "    print('eli5 was not installed, installed and imported')    \n",
    "\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(myModel, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Micha said: \"Again, this is a model. Always understand what you see. The feature \"Special\" have a big impact on the \"Overall\" score. But if you do not know how the \"Special\" feature is calculated. Be careful with predictions. It theory it is posible that other features have a major impact if \"Special\" is not linair. Always consult a soccer speciallist that can motivate the features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how did our model perform?\n",
    "\n",
    "Well, first store the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = myModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize how the predicted values perform against the tested results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or look at the residuals (the difference between the two):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    print('statsmodels already installed, only imported')\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "    print('statsmodels was not installed, installed and imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot((y_test-predictions),bins=50, kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use a metric: (if I'm not mistaken, this is the determination coefficient r-square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the beginning this holdout technique is one of the two main types to split your data. The other is K-Fold.\n",
    "\n",
    "For more info on K-Fold see:\n",
    "\n",
    "https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 and https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><strong>BACK TO THE SLIDES</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values are actually part for the data cleaning. A useful library for this is the <a href=\"https://pypi.org/project/missingno/\">*missingno*</a> library':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import missingno as msno\n",
    "    print('missingno already installed, only imported')\n",
    "except:\n",
    "    !pip install missingno\n",
    "    import missingno as msno\n",
    "    print('missingno was not installed, installed and imported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a quick overview of the count of different variables using the **msno.bar()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(soccerDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale on the top is the proportion of completeness, 0.0 = 0% and 1.0 = 100%. The number on the right shows the number of non-empty scores, which is also the scale on the bottom of the chart.\n",
    "\n",
    "The 'Loaned From' appears to have a very low count.\n",
    "\n",
    "We can also see the results more in a matrix style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(soccerDF,labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each bar is one variable, and is made up of rows, where each row is a case (respondent). \n",
    "\n",
    "White indicates that there is no score. We can see the same bars with low counts as in the bar chart, but also a 'row' perspective From this we can see that many respondents skipped the same questions.\n",
    "\n",
    "And we can of course do some more fancy things with the colors, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(soccerDF.sample(250),color=(0,0.5,0),labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small side note. We have a few columns that are links, or not named at all. We can remove those using 'drop':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF.drop(['Unnamed: 0','Photo','Flag','Club Logo'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
